# LaMMA-P Benchmarking Framework

A production-grade research artifact for evaluating LLM performance in robotics planning tasks, specifically for the LaMMA-P pipeline.

## ğŸ— Directory Structure

```
lamma_p_bench/
â”œâ”€â”€ config.py           # Environment and global configuration
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ llm_client.py   # Swappable LLM adapter (OpenAI/Ollama)
â”‚   â”œâ”€â”€ schema.py       # JSON schema enforcement and validation
â”‚   â””â”€â”€ logger.py       # Benchmarking CSV logger
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ run_floor6_eval.py    # Main evaluation harness
â”‚   â””â”€â”€ visualize_results.py  # Result visualization script
â”œâ”€â”€ testcases/
â”‚   â””â”€â”€ floor6/         # "Floor 6" evaluation data
â”œâ”€â”€ results/            # Benchmark outputs and charts
â”œâ”€â”€ requirements.txt    # Project dependencies
â””â”€â”€ example.env         # Environment variable template
```

## ğŸš€ Getting Started

### 1. Installation

```bash
pip install -r requirements.txt
```

### 2. Configuration

Copy `example.env` to `.env` and configure your providers:

```bash
cp example.env .env
# Edit .env with your LLM_MODEL, OPENAI_API_KEY, etc.
```

### 3. Running Benchmarks

To run the "Floor 6" evaluation for a specific model:

```bash
# Evaluate a local model via Ollama
python evaluation/run_floor6_eval.py --model mistral:7b --provider ollama --trials 10

# Evaluate a cloud model via OpenAI
python evaluation/run_floor6_eval.py --model gpt-4o --provider openai --trials 5
```

### 4. Visualizing Results

After running benchmarks, generate comparison charts:

```bash
python evaluation/visualize_results.py
```

Results will be saved in the `results/` directory as CSVs and PNG charts.

## ğŸ”¬ Research Features

- **JSON Schema Enforcement**: Uses Pydantic to ensure LLM outputs always match the required robotics task structure.
- **Deterministic Evaluation**: Fixed test cases and temperature = 0.
- **Hybrid Fallback**: Optional automatic fallback to cloud LLMs if local models fail validation.
- **Reproducibility**: Comprehensive CSV logging including latency, retry counts, and validity rates.
